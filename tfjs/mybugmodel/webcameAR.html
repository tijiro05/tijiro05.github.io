<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>動植物識別AR（マーカーレス）</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.0.0/dist/tf.min.js"></script>
  <script src="https://aframe.io/releases/1.2.0/aframe.min.js"></script>
  <style>
    body { font-family: sans-serif; margin: 0; overflow: hidden; }
    #overlay {
      position: absolute; top: 10px; left: 10px; background: rgba(255,255,255,0.9);
      padding: 10px; border-radius: 8px; z-index: 10;
    }
    #confidenceBar { height: 10px; background: #eee; margin-top: 5px; width: 100%; border-radius: 5px; overflow: hidden; }
    #confidenceFill { height: 100%; width: 0%; background: limegreen; transition: width 0.5s; }
   
  </style>
</head>
<body>
  <div id="overlay">
    <p id="status">モデル読み込み中...</p>
    <p id="result">分類結果がここに表示されます</p>
    <div id="confidenceBar"><div id="confidenceFill"></div></div>
    <button id="predictButton">分類する</button>
  </div>

  <video id="webcam" autoplay playsinline muted></video>

  <a-scene embedded vr-mode-ui="enabled: false">
    <!-- カメラ映像を背景に -->
    <a-assets>
      <video id="videoTexture" autoplay loop muted></video>
    </a-assets>

    <a-sky id="sky" radius="10" theta-length="180" material="shader: flat; src: #videoTexture;"></a-sky>

    <!-- シロツメクサ認識時の3Dテキスト -->
    <a-entity id="cloverText" visible="false"
              text="value: This is White Clover;
                    color: #000; width:100; align: center"
              position="0 1.7 -2"
              scale="1 1 1">
    </a-entity>

    <a-entity camera look-controls></a-entity>
  </a-scene>

  <script>
    const modelPath = 'model/model.json';
    const classLabels = ['蚊', 'バッタ', 'てんとう虫', 'アブラムシ', 'カメムシ', 'アリ', 'ハチ', 'チョウ', 'トンボ', 'セミ', 'シロツメクサ'];
    let model;
    const video = document.getElementById('webcam');
    const videoTexture = document.getElementById('videoTexture');

    // モデル読み込み
    tf.loadLayersModel(modelPath).then((loadedModel) => {
      model = loadedModel;
      document.getElementById('status').textContent = 'モデル読み込み完了。カメラに映して「分類する」を押してください。';
    });

    // カメラ起動
    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' }, audio: false });
      video.srcObject = stream;
      videoTexture.srcObject = stream;
    }

    setupCamera();

    // 分類ボタン押下時
    document.getElementById('predictButton').addEventListener('click', () => {
      if (!model) return;

      const tempCanvas = document.createElement('canvas');
      tempCanvas.width = 224;
      tempCanvas.height = 224;
      const ctx = tempCanvas.getContext('2d');
      ctx.drawImage(video, 0, 0, 224, 224);

      const imageData = ctx.getImageData(0, 0, 224, 224);
      const tensor = tf.browser.fromPixels(imageData).toFloat().div(tf.scalar(255)).expandDims();

      model.predict(tensor).array().then(predictions => {
        const scores = predictions[0];

        const topResults = scores
          .map((score, i) => ({ label: classLabels[i], score }))
          .sort((a, b) => b.score - a.score)
          .slice(0, 3);

        const resultText = topResults.map(r =>
          `${r.label}（${(r.score * 100).toFixed(1)}%）`
        ).join(' / ');

        document.getElementById('result').textContent = `分類結果: ${resultText}`;
        document.getElementById('status').textContent = '分類完了';
        document.getElementById('confidenceFill').style.width = `${(topResults[0].score * 100).toFixed(1)}%`;
        console.log(topResults[0].score);
        console.log(topResults[0].label);

        if (topResults[0].label === 'シロツメクサ' && topResults[0].score > 0.85) {
          document.getElementById('cloverText').setAttribute('visible', 'true');
        }
      });
    });
  </script>
</body>
</html>
