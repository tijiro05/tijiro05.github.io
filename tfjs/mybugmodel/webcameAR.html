<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <title>動植物識別AR（マーカーレス）</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.0.0/dist/tf.min.js"></script>
  <script src="https://aframe.io/releases/1.7.0/aframe.min.js"></script>
  <style>
    body { font-family: sans-serif; margin: 0; overflow: hidden; }
    #overlay {
      position: absolute; top: 10px; left: 10px; background: rgba(255,255,255,0.9);
      padding: 10px; border-radius: 8px; z-index: 10;
    }
    #confidenceBar { height: 10px; background: #eee; margin-top: 5px; width: 100%; border-radius: 5px; overflow: hidden; }
    #confidenceFill { height: 100%; width: 0%; background: limegreen; transition: width 0.5s; }
  </style>
</head>
<body>
  <div id="overlay">
    <p id="status">モデル読み込み中...</p>
    <p id="result">分類結果がここに表示されます</p>
    <div id="confidenceBar"><div id="confidenceFill"></div></div>
    <button id="predictButton">分類する</button>
  </div>

  <video id="webcam" autoplay playsinline muted></video>

  <a-scene embedded vr-mode-ui="enabled: false">
    <!-- カメラ映像を背景に -->
    <a-assets>
      <video id="videoTexture" autoplay loop muted></video>
    </a-assets>

    <!-- 背景をカメラ映像として表示 -->
    <a-video src="#videoTexture" position="0 1.6 -3" rotation="0 0 0" width="4" height="3"></a-video>

    <!-- シロツメクサ認識時の3Dテキスト -->
    <a-entity id="cloverText" visible="false" position="0 1.6 -1">
      <a-plane color="#FFFFFF" height="0.5" width="2"></a-plane>
      <a-text value="This is White Clover" color="#000000" align="center" position="0 0 -1" width="2"></a-text>
    </a-entity>

    <a-entity camera look-controls></a-entity>
  </a-scene>

  <script>
    const modelPath = 'model/model.json';
    const classLabels = ['蚊', 'バッタ', 'てんとう虫', 'アブラムシ', 'カメムシ', 'アリ', 'ハチ', 'チョウ', 'トンボ', 'セミ', 'シロツメクサ'];
    let model;
    const video = document.getElementById('webcam');
    const videoTexture = document.getElementById('videoTexture');

    // モデル読み込み
    tf.loadLayersModel(modelPath).then((loadedModel) => {
      model = loadedModel;
      document.getElementById('status').textContent = 'モデル読み込み完了。カメラに映して「分類する」を押してください。';
    });

    // カメラ起動
    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' }, audio: false });
      video.srcObject = stream;
      videoTexture.srcObject = stream;
    }

    setupCamera();

    // 分類ボタン押下時
    document.getElementById('predictButton').addEventListener('click', () => {
      if (!model) return;

      const tempCanvas = document.createElement('canvas');
      tempCanvas.width = 224;
      tempCanvas.height = 224;
      const ctx = tempCanvas.getContext('2d');
      ctx.drawImage(video, 0, 0, 224, 224);

      const imageData = ctx.getImageData(0, 0, 224, 224);
      const tensor = tf.browser.fromPixels(imageData).toFloat().div(tf.scalar(255)).expandDims();

      model.predict(tensor).array().then(predictions => {
        const scores = predictions[0];

        const topResults = scores
          .map((score, i) => ({ label: classLabels[i], score }))
          .sort((a, b) => b.score - a.score)
          .slice(0, 3);

        const resultText = topResults.map(r =>
          `${r.label}（${(r.score * 100).toFixed(1)}%）`
        ).join(' / ');

        document.getElementById('result').textContent = `分類結果: ${resultText}`;
        document.getElementById('status').textContent = '分類完了';
        document.getElementById('confidenceFill').style.width = `${(topResults[0].score * 100).toFixed(1)}%`;

        // シロツメクサ認識時にテキスト表示
        if ((topResults[0].label === 'シロツメクサ') && (topResults[0].score > 0.5)) {
          document.getElementById('cloverText').setAttribute('visible', 'true');
        } else {
          document.getElementById('cloverText').setAttribute('visible', 'false');
        }
      });
    });
  </script>
</body>
</html>
